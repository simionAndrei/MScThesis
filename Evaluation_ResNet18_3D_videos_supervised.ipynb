{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from temporal_transform import TemporalRandomCrop, TemporalCenterCrop\n",
    "from resnet3d import generate_model\n",
    "from ucf101 import get_ucf_dataset\n",
    "from hmdb51 import get_hmdb_dataset\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(show = True, html_output = True, config_file = \"config.txt\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "frame_size = logger.config_dict['frame_resize']\n",
    "spatial_transform = transforms.Compose([transforms.Resize((frame_size, frame_size)), transforms.ToTensor()])\n",
    "logger.log(\"Frames resized to {}x{}\".format(frame_size, frame_size))\n",
    "\n",
    "sampling_method_str = logger.config_dict['sampling_method']\n",
    "if \"rand\" in sampling_method_str:\n",
    "    crop_size = int(sampling_method_str.replace(\"rand\", \"\"))\n",
    "    sampling_method = TemporalRandomCrop(size = crop_size)\n",
    "    logger.log(\"Sampling strategy selecting {} consecutives frames from a random index\".format(\n",
    "      crop_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(logger.data_folder, \n",
    "                          logger.config_dict['video_folder'], logger.config_dict['frame_folder'])\n",
    "annotation_path = logger.get_data_file(logger.config_dict['annotation_file'])\n",
    "batch_size  = logger.config_dict['batch_size']\n",
    "num_workers = logger.config_dict['num_workers']\n",
    "dataset_type = logger.config_dict['dataset_type']\n",
    "\n",
    "if dataset_type == \"ucf101\":\n",
    "    train_dataset = get_ucf_dataset(video_path, annotation_path, \"training\", sampling_method, \n",
    "                                    spatial_transform, temporal_transform = \"None\",\n",
    "                                    stack_clip = True, is_simclr_transform = False, \n",
    "                                    apply_same_per_clip = True)\n",
    "    test_dataset = get_ucf_dataset(video_path, annotation_path, \"validation\", sampling_method, \n",
    "                                   spatial_transform, temporal_transform = \"None\",\n",
    "                                   stack_clip = True, is_simclr_transform = False, \n",
    "                                   apply_same_per_clip = True)\n",
    "elif dataset_type == \"hmdb51\":\n",
    "    train_dataset = get_hmdb_dataset(video_path, annotation_path, \"training\", sampling_method, \n",
    "                                     spatial_transform, temporal_transform = \"None\",\n",
    "                                     stack_clip = True, is_simclr_transform = False, \n",
    "                                     apply_same_per_clip = True)\n",
    "    test_dataset = get_hmdb_dataset(video_path, annotation_path, \"validation\", sampling_method, \n",
    "                                    spatial_transform, temporal_transform = \"None\",\n",
    "                                    stack_clip = True, is_simclr_transform = False, \n",
    "                                    apply_same_per_clip = True)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle = False, drop_last = False, \n",
    "                           num_workers = num_workers)\n",
    "logger.log(\"{} Train data loaded with {} clips\".format(dataset_type.upper(), len(train_dataset)))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, drop_last = False,\n",
    "                         num_workers = num_workers)\n",
    "logger.log(\"{} Test data loaded with {} clips\".format(dataset_type.upper(), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = logger.config_dict['base_convnet']\n",
    "num_classes = logger.config_dict['num_classes'] \n",
    "model = generate_model(model_depth = 18, n_classes = num_classes, add_projection_layers = False)\n",
    "logger.log(\"Model {} 3D loaded {}\".format(base_model, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_epoch = logger.config_dict['model_checkpoint_epoch']\n",
    "checkpoint_file  = logger.config_dict['model_checkpoint_file']\n",
    "logger.log(\"Loading supervised model weights at epoch {} from {}\".format(checkpoint_epoch, checkpoint_file))\n",
    "checkpoint = torch.load(logger.get_model_file(checkpoint_file))\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "except:\n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Identity()\n",
    "logger.log(\"Removing last FC layer\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_avail = torch.cuda.is_available()\n",
    "logger.log(\"GPU available: {}\".format(gpu_avail))\n",
    "if gpu_avail:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "logger.log(\"Generating supervised features from training data ...\")\n",
    "X_train_feature = []\n",
    "y_train = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "        if gpu_avail:\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "        features = model(inputs)\n",
    "        X_train_feature.extend(features.detach().cpu().numpy())\n",
    "        y_train.extend(labels.numpy())\n",
    "\n",
    "    \n",
    "X_train_feature = np.array(X_train_feature)\n",
    "logger.log(\"Train features of shape: {}\".format(X_train_feature.shape), show_time = True)\n",
    "\n",
    "\n",
    "'''\n",
    "train_feats_filename  = base_model + \"_sup_\" + str(frame_size) + \"f_\" + \"x_train_feats\"\n",
    "train_feats_filename += \"_\" + dataset_type + \"_\" + time_crop_type + \"_3d.npy\"\n",
    "\n",
    "train_feats_filename = logger.get_output_file(train_feats_filename)\n",
    "logger.log(\"Saving training data supervised features at {}\".format(train_feats_filename))\n",
    "np.save(train_feats_filename, X_train_feature)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "logger.log(\"Generating supervised features from testing data ...\")\n",
    "X_test_feature = []\n",
    "y_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in tqdm(enumerate(test_loader)):\n",
    "        if gpu_avail:\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "        features = model(inputs)\n",
    "        X_test_feature.extend(features.detach().cpu().numpy())\n",
    "        y_test.extend(labels.numpy())\n",
    "\n",
    "X_test_feature = np.array(X_test_feature)\n",
    "logger.log(\"Test features of shape: {}\".format(X_test_feature.shape), show_time = True)\n",
    "\n",
    "\n",
    "'''\n",
    "test_feats_filename  = base_model + \"_sup_\" + str(frame_size) + \"f_\" + \"x_test_feats\"\n",
    "test_feats_filename += \"_\" + dataset_type + \"_\" + time_crop_type + \"_3d.npy\"\n",
    "\n",
    "test_feats_filename = logger.get_output_file(test_feats_filename)\n",
    "logger.log(\"Saving testing data supervised features at {}\".format(test_feats_filename))\n",
    "np.save(test_feats_filename, X_test_feature)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=2000, solver='lbfgs', C=1.0)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train_feature)\n",
    "\n",
    "logger.log(\"Start training LogisticRegression on supervised features ...\")\n",
    "clf.fit(scaler.transform(X_train_feature), y_train)\n",
    "logger.log(\"Finished training\", show_time = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log(\"Supervised Resnet18 3D feature evaluation on {}\".format(dataset_type))\n",
    "logger.log(\"Train score: {:.3f}\".format(clf.score(scaler.transform(X_train_feature), y_train)))\n",
    "logger.log(\"Test score: {:.3f}\".format(clf.score(scaler.transform(X_test_feature), y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
