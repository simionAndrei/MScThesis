{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn.preprocessing import normalize\n",
    "import sklearn.metrics.pairwise as smp\n",
    " \n",
    "def cosine_distance(row1, row2):\n",
    "    return 1 - spatial.distance.cosine(row1, row2)\n",
    "\n",
    "def euclidean_distance(row1, row2):\n",
    "    return np.linalg.norm(np.array(row1) - np.array(row2))\n",
    " \n",
    "def get_neighbors(train, test_row, distance, num_neighbors):\n",
    "    \n",
    "    distances = list()\n",
    "    for i, train_row in enumerate(train):\n",
    "        dist = distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "\n",
    "    sorted_idxs = [x for x,y in sorted(enumerate(distances), key = lambda x: -x[1][1])]\n",
    "    return sorted_idxs[:num_neighbors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.07.28-19:34:25 Initialize the logger\n",
      "2020.07.28-19:34:25 Create logs folder logs\n",
      "2020.07.28-19:34:25 Create log file logs\\2020-07-28_19_34_25_log0.html\n",
      "2020.07.28-19:34:25 Read config file config.txt\n",
      "2020.07.28-19:34:25 Create models folder models\n",
      "2020.07.28-19:34:25 Create output folder output\n",
      "2020.07.28-19:34:25 Create data folder data\n",
      "[2020.07.28-19:34:25] Frames resized to 224x224\n",
      "[2020.07.28-19:34:25] Sampling strategy selecting 16 consecutives frames from a random index\n",
      "dataset loading [0/9537]\n",
      "dataset loading [1000/9537]\n",
      "dataset loading [2000/9537]\n",
      "dataset loading [3000/9537]\n",
      "dataset loading [4000/9537]\n",
      "dataset loading [5000/9537]\n",
      "dataset loading [6000/9537]\n",
      "dataset loading [7000/9537]\n",
      "dataset loading [8000/9537]\n",
      "dataset loading [9000/9537]\n",
      "dataset loading [0/3783]\n",
      "dataset loading [1000/3783]\n",
      "dataset loading [2000/3783]\n",
      "dataset loading [3000/3783]\n",
      "[2020.07.28-19:35:47] UCF101 Train data loaded with 9537 clips\n",
      "[2020.07.28-19:35:47] UCF101 Test data loaded with 3783 clips\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from distutils import util\n",
    "\n",
    "from temporal_transform import TemporalRandomCrop, TemporalCenterCrop\n",
    "from resnet3d import generate_model\n",
    "from ucf101 import get_ucf_dataset\n",
    "from hmdb51 import get_hmdb_dataset\n",
    "from logger import Logger\n",
    "\n",
    "logger = Logger(show = True, html_output = True, config_file = \"config.txt\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "frame_size = logger.config_dict['frame_resize']\n",
    "spatial_transform = transforms.Compose([transforms.Resize((frame_size, frame_size)), transforms.ToTensor()])\n",
    "logger.log(\"Frames resized to {}x{}\".format(frame_size, frame_size))\n",
    "\n",
    "sampling_method_str = logger.config_dict['sampling_method']\n",
    "if \"rand\" in sampling_method_str:\n",
    "    crop_size = int(sampling_method_str.replace(\"rand\", \"\"))\n",
    "    sampling_method = TemporalRandomCrop(size = crop_size)\n",
    "    sampling_method = TemporalCenterCrop(size = crop_size)\n",
    "    logger.log(\"Sampling strategy selecting {} consecutives frames from a random index\".format(\n",
    "      crop_size))\n",
    "\n",
    "video_path = os.path.join(logger.config_dict['data_folder'], \n",
    "                          logger.config_dict['video_folder'], logger.config_dict['frame_folder'])\n",
    "annotation_path = logger.get_data_file(logger.config_dict['annotation_file'])\n",
    "batch_size  = logger.config_dict['batch_size']\n",
    "num_workers = logger.config_dict['num_workers']\n",
    "dataset_type = logger.config_dict['dataset_type']\n",
    "\n",
    "if dataset_type == \"ucf101\":\n",
    "    train_dataset = get_ucf_dataset(video_path, annotation_path, \"training\", sampling_method, \n",
    "                                    spatial_transform, temporal_transform = \"None\",\n",
    "                                    stack_clip = True, is_simclr_transform = False, \n",
    "                                    apply_same_per_clip = True)\n",
    "    test_dataset = get_ucf_dataset(video_path, annotation_path, \"validation\", sampling_method, \n",
    "                                   spatial_transform, temporal_transform = \"None\",\n",
    "                                   stack_clip = True, is_simclr_transform = False, \n",
    "                                   apply_same_per_clip = True)\n",
    "elif dataset_type == \"hmdb51\":\n",
    "    train_dataset = get_hmdb_dataset(video_path, annotation_path, \"training\", sampling_method, \n",
    "                                     spatial_transform, temporal_transform = \"None\",\n",
    "                                     stack_clip = True, is_simclr_transform = False, \n",
    "                                     apply_same_per_clip = True)\n",
    "    test_dataset = get_hmdb_dataset(video_path, annotation_path, \"validation\", sampling_method, \n",
    "                                    spatial_transform, temporal_transform = \"None\",\n",
    "                                    stack_clip = True, is_simclr_transform = False, \n",
    "                                    apply_same_per_clip = True)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle = False, drop_last = False, \n",
    "                           num_workers = num_workers)\n",
    "logger.log(\"{} Train data loaded with {} clips\".format(dataset_type.upper(), len(train_dataset)))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, drop_last = False,\n",
    "                         num_workers = num_workers)\n",
    "logger.log(\"{} Test data loaded with {} clips\".format(dataset_type.upper(), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinet_test = np.load(logger.get_data_file(\"kinet_ucf01_test.npy\"))\n",
    "videoSimCLR_test = np.load(logger.get_data_file(\"ucf01_test.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2666, 1045, 2667, 971, 2665, 1256, 3646]\n",
      "[2666, 2665, 2671, 2668, 2683, 2660, 2662]\n"
     ]
    }
   ],
   "source": [
    "nei = get_neighbors(videoSimCLR_test, videoSimCLR_test[2666], cosine_distance, 7)\n",
    "kinet_nei = get_neighbors(kinet_test, kinet_test[2666], cosine_distance, 7)\n",
    "print(nei)\n",
    "print(kinet_nei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [05:06,  5.11s/it]\n"
     ]
    }
   ],
   "source": [
    "videos = {}\n",
    "#nei = kinet_nei\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in tqdm(enumerate(test_loader)):\n",
    "      if i*batch_size <= nei[0] and nei[0] < (i+1)*batch_size:\n",
    "        videos[nei[0]] = inputs[nei[0] - i*batch_size]\n",
    "      if i*batch_size <= nei[1] and nei[1] < (i+1)*batch_size:\n",
    "        videos[nei[1]] = inputs[nei[1] - i*batch_size]\n",
    "      if i*batch_size <= nei[2] and nei[2] < (i+1)*batch_size:\n",
    "        videos[nei[2]] = inputs[nei[2] - i*batch_size]\n",
    "      if i*batch_size <= nei[3] and nei[3] < (i+1)*batch_size:\n",
    "        videos[nei[3]] = inputs[nei[3] - i*batch_size]\n",
    "      if i*batch_size <= nei[4] and nei[4] < (i+1)*batch_size:\n",
    "        videos[nei[4]] = inputs[nei[4] - i*batch_size]\n",
    "      if i*batch_size <= nei[5] and nei[5] < (i+1)*batch_size:\n",
    "        videos[nei[5]] = inputs[nei[5] - i*batch_size]\n",
    "      if i*batch_size <= nei[6] and nei[6] < (i+1)*batch_size:\n",
    "        videos[nei[6]] = inputs[nei[6] - i*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "for i in range(4):\n",
    "  img = videos[nei[idx]].permute(1, 0, 2, 3)[i*4]\n",
    "  save_image(img, \"output/kinet\" + \"f\" + str(i*4) + \"_\" + str(nei[idx]) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.load(logger.get_data_file(\"y_test_labels.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nei1 = [ 666, 1286,  355, 2441]\n",
    "nei2 = [ 777, 1334,  778, 2637]\n",
    "nei3  = [ 333, 2853,  338, 3518]\n",
    "nei = [1000, 2745, 2472, 2063]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
