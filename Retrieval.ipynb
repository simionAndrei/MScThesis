{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn.preprocessing import normalize\n",
    "import sklearn.metrics.pairwise as smp\n",
    " \n",
    "def cosine_distance(row1, row2):\n",
    "    return 1 - spatial.distance.cosine(row1, row2)\n",
    "\n",
    "def euclidean_distance(row1, row2):\n",
    "    return np.linalg.norm(np.array(row1) - np.array(row2))\n",
    " \n",
    "def get_neighbors(train, test_row, distance, num_neighbors):\n",
    "    \n",
    "    distances = list()\n",
    "    for i, train_row in enumerate(train):\n",
    "        dist = distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "\n",
    "    sorted_idxs = [x for x,y in sorted(enumerate(distances), key = lambda x: -x[1][1])]\n",
    "    return sorted_idxs[:num_neighbors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from distutils import util\n",
    "\n",
    "from temporal_transform import TemporalRandomCrop, TemporalCenterCrop\n",
    "from resnet3d import generate_model\n",
    "from ucf101 import get_ucf_dataset\n",
    "from hmdb51 import get_hmdb_dataset\n",
    "from logger import Logger\n",
    "\n",
    "logger = Logger(show = True, html_output = True, config_file = \"config.txt\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "frame_size = logger.config_dict['frame_resize']\n",
    "spatial_transform = transforms.Compose([transforms.Resize((frame_size, frame_size)), transforms.ToTensor()])\n",
    "logger.log(\"Frames resized to {}x{}\".format(frame_size, frame_size))\n",
    "\n",
    "sampling_method_str = logger.config_dict['sampling_method']\n",
    "if \"rand\" in sampling_method_str:\n",
    "    crop_size = int(sampling_method_str.replace(\"rand\", \"\"))\n",
    "    sampling_method = TemporalRandomCrop(size = crop_size)\n",
    "    sampling_method = TemporalCenterCrop(size = crop_size)\n",
    "    logger.log(\"Sampling strategy selecting {} consecutives frames from a random index\".format(\n",
    "      crop_size))\n",
    "\n",
    "video_path = os.path.join(logger.config_dict['data_folder'], \n",
    "                          logger.config_dict['video_folder'], logger.config_dict['frame_folder'])\n",
    "annotation_path = logger.get_data_file(logger.config_dict['annotation_file'])\n",
    "batch_size  = logger.config_dict['batch_size']\n",
    "num_workers = logger.config_dict['num_workers']\n",
    "dataset_type = logger.config_dict['dataset_type']\n",
    "\n",
    "if dataset_type == \"ucf101\":\n",
    "    train_dataset = get_ucf_dataset(video_path, annotation_path, \"training\", sampling_method, \n",
    "                                    spatial_transform, temporal_transform = \"None\",\n",
    "                                    stack_clip = True, is_simclr_transform = False, \n",
    "                                    apply_same_per_clip = True)\n",
    "    test_dataset = get_ucf_dataset(video_path, annotation_path, \"validation\", sampling_method, \n",
    "                                   spatial_transform, temporal_transform = \"None\",\n",
    "                                   stack_clip = True, is_simclr_transform = False, \n",
    "                                   apply_same_per_clip = True)\n",
    "elif dataset_type == \"hmdb51\":\n",
    "    train_dataset = get_hmdb_dataset(video_path, annotation_path, \"training\", sampling_method, \n",
    "                                     spatial_transform, temporal_transform = \"None\",\n",
    "                                     stack_clip = True, is_simclr_transform = False, \n",
    "                                     apply_same_per_clip = True)\n",
    "    test_dataset = get_hmdb_dataset(video_path, annotation_path, \"validation\", sampling_method, \n",
    "                                    spatial_transform, temporal_transform = \"None\",\n",
    "                                    stack_clip = True, is_simclr_transform = False, \n",
    "                                    apply_same_per_clip = True)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle = False, drop_last = False, \n",
    "                           num_workers = num_workers)\n",
    "logger.log(\"{} Train data loaded with {} clips\".format(dataset_type.upper(), len(train_dataset)))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, drop_last = False,\n",
    "                         num_workers = num_workers)\n",
    "logger.log(\"{} Test data loaded with {} clips\".format(dataset_type.upper(), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinet_test = np.load(logger.get_data_file(\"kinet_ucf01_test.npy\"))\n",
    "videoSimCLR_test = np.load(logger.get_data_file(\"ucf01_test.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nei = get_neighbors(videoSimCLR_test, videoSimCLR_test[2666], cosine_distance, 7)\n",
    "kinet_nei = get_neighbors(kinet_test, kinet_test[2666], cosine_distance, 7)\n",
    "print(nei)\n",
    "print(kinet_nei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = {}\n",
    "#nei = kinet_nei\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in tqdm(enumerate(test_loader)):\n",
    "      if i*batch_size <= nei[0] and nei[0] < (i+1)*batch_size:\n",
    "        videos[nei[0]] = inputs[nei[0] - i*batch_size]\n",
    "      if i*batch_size <= nei[1] and nei[1] < (i+1)*batch_size:\n",
    "        videos[nei[1]] = inputs[nei[1] - i*batch_size]\n",
    "      if i*batch_size <= nei[2] and nei[2] < (i+1)*batch_size:\n",
    "        videos[nei[2]] = inputs[nei[2] - i*batch_size]\n",
    "      if i*batch_size <= nei[3] and nei[3] < (i+1)*batch_size:\n",
    "        videos[nei[3]] = inputs[nei[3] - i*batch_size]\n",
    "      if i*batch_size <= nei[4] and nei[4] < (i+1)*batch_size:\n",
    "        videos[nei[4]] = inputs[nei[4] - i*batch_size]\n",
    "      if i*batch_size <= nei[5] and nei[5] < (i+1)*batch_size:\n",
    "        videos[nei[5]] = inputs[nei[5] - i*batch_size]\n",
    "      if i*batch_size <= nei[6] and nei[6] < (i+1)*batch_size:\n",
    "        videos[nei[6]] = inputs[nei[6] - i*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "for i in range(4):\n",
    "  img = videos[nei[idx]].permute(1, 0, 2, 3)[i*4]\n",
    "  save_image(img, \"output/kinet\" + \"f\" + str(i*4) + \"_\" + str(nei[idx]) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.load(logger.get_data_file(\"y_test_labels.npy\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
